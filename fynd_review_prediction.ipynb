# ================================================================
# COMPLETE REFINED & CORRECTED REVIEW PREDICTION CODE
# Copy each cell sequentially into your Jupyter notebook
# ================================================================

# ================================================================
# CELL 1: Install Required Packages
# ================================================================
!pip install -q pandas>=2.0.0 numpy>=1.24.0 python-dotenv>=1.0.0 openai>=1.0.0 tenacity>=8.2.0 jsonschema>=4.17.0 matplotlib>=3.7.0 seaborn>=0.12.0 scikit-learn>=1.3.0 jupyter>=1.0.0 ipykernel>=6.25.0


# ================================================================
# CELL 2: Import Core Libraries
# ================================================================
import pandas as pd
import numpy as np
import json
import os
import time
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, asdict
from datetime import datetime
from collections import defaultdict


# ================================================================
# CELL 3: Suppress Warnings
# ================================================================
import warnings
warnings.filterwarnings('ignore')


# ================================================================
# CELL 4: Import Visualization Libraries
# ================================================================
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, mean_absolute_error


# ================================================================
# CELL 5: Import LLM and Validation Libraries
# ================================================================
from openai import OpenAI
from tenacity import retry, stop_after_attempt, wait_exponential
import jsonschema


# ================================================================
# CELL 6: Configure Visualization Settings
# ================================================================
sns.set_style('whitegrid')
plt.rcParams['figure.figsize'] = (12, 6)


# ================================================================
# CELL 7: Configuration Class
# ================================================================
@dataclass
class Config:
    """Production-grade configuration management"""

    # LLM Settings
    MODEL_NAME: str = "gemini-1.5-flash"  # Cost-effective for production
    TEMPERATURE: float = 0.1  # Low temperature for consistency
    MAX_TOKENS: int = 300

    # Data Settings
    SAMPLE_SIZE: int = 200
    RANDOM_SEED: int = 42

    # Retry Settings
    MAX_RETRIES: int = 3
    RETRY_WAIT_MIN: int = 1
    RETRY_WAIT_MAX: int = 10

    # Validation
    VALID_STARS: List[int] = None

    def __post_init__(self):
        self.VALID_STARS = [1, 2, 3, 4, 5]

config = Config()

# JSON Schema for output validation
OUTPUT_SCHEMA = {
    "type": "object",
    "properties": {
        "predicted_stars": {"type": "integer", "minimum": 1, "maximum": 5},
        "explanation": {"type": "string", "minLength": 10}
    },
    "required": ["predicted_stars", "explanation"]
}


# ================================================================
# CELL 8: DataLoader Class
# ================================================================
class DataLoader:
    """Production-grade data loading with validation and sampling"""

    def __init__(self, config: Config):
        self.config = config
        self.logger = self._setup_logger()

    def _setup_logger(self) -> Dict[str, List]:
        """Initialize logging structure"""
        return {
            'timestamp': [],
            'event': [],
            'details': []
        }

    def load_and_sample(self, filepath: str) -> pd.DataFrame:
        """Load dataset with stratified sampling"""
        try:
            # Load data
            df = pd.read_csv(filepath)
            self._log('data_loaded', f'Total rows: {len(df)}')

            # Validate columns
            required_cols = ['text', 'stars']
            if not all(col in df.columns for col in required_cols):
                raise ValueError(f"Missing required columns: {required_cols}")

            # Clean data
            df = df.dropna(subset=required_cols)
            df = df[df['stars'].isin(self.config.VALID_STARS)]
            df['text'] = df['text'].astype(str).str.strip()
            df = df[df['text'].str.len() > 10]  # Filter very short reviews

            self._log('data_cleaned', f'Clean rows: {len(df)}')

            # Stratified sampling for balanced evaluation
            sampled_df = df.groupby('stars', group_keys=False).apply(
                lambda x: x.sample(min(len(x), self.config.SAMPLE_SIZE // 5),
                                 random_state=self.config.RANDOM_SEED)
            ).reset_index(drop=True)

            self._log('data_sampled', f'Sampled rows: {len(sampled_df)}')

            # Display distribution
            print("\nüìä Rating Distribution:")
            print(sampled_df['stars'].value_counts().sort_index())

            return sampled_df

        except Exception as e:
            self._log('error', str(e))
            raise

    def _log(self, event: str, details: str):
        """Log events for observability"""
        self.logger['timestamp'].append(datetime.now())
        self.logger['event'].append(event)
        self.logger['details'].append(details)


# ================================================================
# CELL 9: Mount Google Drive
# ================================================================
from google.colab import drive
drive.mount('/content/drive')


# ================================================================
# CELL 10: LLMClient Class (FIXED VERSION)
# ================================================================
class LLMClient:
    """Production-grade LLM client with retry, validation, and monitoring"""

    def __init__(self, config: Config):
        self.config = config
        # Replace with your actual API key or use environment variable
        self.client = OpenAI(
            api_key=os.getenv('GEMINI_API_KEY', 'YOUR_API_KEY_HERE'),
            base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
        )
        self.metrics = self._init_metrics()

    def _init_metrics(self) -> Dict:
        """Initialize metrics tracking"""
        return {
            'total_calls': 0,
            'successful_calls': 0,
            'failed_calls': 0,
            'total_tokens': 0,
            'latencies': [],
            'json_valid': 0,
            'json_invalid': 0
        }

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(min=1, max=10)
    )
    def call_llm(self, prompt: str) -> Dict[str, Any]:
        """Call LLM with retry logic and validation"""
        start_time = time.time()
        self.metrics['total_calls'] += 1

        try:
            response = self.client.chat.completions.create(
                model=self.config.MODEL_NAME,
                messages=[{"role": "user", "content": prompt}],
                temperature=self.config.TEMPERATURE,
                max_tokens=self.config.MAX_TOKENS,
                response_format={"type": "json_object"}
            )

            # Extract and parse response
            content = response.choices[0].message.content
            result = json.loads(content)

            # Validate schema
            jsonschema.validate(instance=result, schema=OUTPUT_SCHEMA)

            # Ensure predicted_stars is an integer
            if not isinstance(result['predicted_stars'], int):
                result['predicted_stars'] = int(result['predicted_stars'])
            
            # Clamp to valid range [1, 5]
            result['predicted_stars'] = max(1, min(5, result['predicted_stars']))

            # Update metrics
            self.metrics['successful_calls'] += 1
            self.metrics['json_valid'] += 1
            if hasattr(response, 'usage') and response.usage:
                self.metrics['total_tokens'] += response.usage.total_tokens
            self.metrics['latencies'].append(time.time() - start_time)

            return result

        except json.JSONDecodeError as e:
            print(f"‚ö†Ô∏è JSON parsing failed: {e}")
            self.metrics['json_invalid'] += 1
            self.metrics['failed_calls'] += 1
            return {"predicted_stars": 3, "explanation": "JSON parsing failed"}

        except jsonschema.ValidationError as e:
            print(f"‚ö†Ô∏è Schema validation failed: {e}")
            self.metrics['json_invalid'] += 1
            self.metrics['failed_calls'] += 1
            return {"predicted_stars": 3, "explanation": "Schema validation failed"}

        except Exception as e:
            self.metrics['failed_calls'] += 1
            print(f"‚ùå LLM call failed: {str(e)}")
            return {"predicted_stars": 3, "explanation": f"Error: {str(e)}"}

    def get_metrics_summary(self) -> Dict:
        """Get performance metrics"""
        total = max(self.metrics['total_calls'], 1)
        return {
            'total_calls': self.metrics['total_calls'],
            'success_rate': self.metrics['successful_calls'] / total,
            'json_validity_rate': self.metrics['json_valid'] / total,
            'avg_latency': np.mean(self.metrics['latencies']) if self.metrics['latencies'] else 0,
            'total_tokens': self.metrics['total_tokens']
        }


# ================================================================
# CELL 11: Prompt Strategy Classes
# ================================================================
from abc import ABC, abstractmethod

class PromptStrategy(ABC):
    """Abstract base class for prompt strategies"""

    def __init__(self, name: str, description: str):
        self.name = name
        self.description = description

    @abstractmethod
    def generate_prompt(self, review_text: str) -> str:
        """Generate prompt for the given review"""
        pass

    def __repr__(self):
        return f"{self.name}: {self.description}"


class ZeroShotStrategy(PromptStrategy):
    """Zero-Shot Prompting - Direct instruction without examples"""

    def __init__(self):
        super().__init__(
            name="Zero-Shot",
            description="Direct classification without examples"
        )

    def generate_prompt(self, review_text: str) -> str:
        return f"""You are a rating prediction system. Analyze the following Yelp review and predict the star rating (1-5 stars).

Review: "{review_text}"

Provide your prediction in JSON format:
{{
    "predicted_stars": <1-5>,
    "explanation": "<brief reasoning>"
}}

Consider sentiment, specific complaints/praises, and overall tone."""


class FewShotStrategy(PromptStrategy):
    """Few-Shot Prompting - Learning from examples"""

    def __init__(self):
        super().__init__(
            name="Few-Shot",
            description="Classification with 5 example demonstrations"
        )
        self.examples = [
            {
                "review": "Absolutely terrible service. Food was cold and the staff was rude. Never coming back!",
                "stars": 1,
                "explanation": "Extremely negative sentiment with multiple complaints about service and food quality"
            },
            {
                "review": "Not great. The food was okay but overpriced. Wouldn't recommend.",
                "stars": 2,
                "explanation": "Negative overall with mediocre food quality and value concerns"
            },
            {
                "review": "Decent place. Nothing special but nothing terrible either. Average experience.",
                "stars": 3,
                "explanation": "Neutral sentiment indicating an average, unremarkable experience"
            },
            {
                "review": "Really enjoyed my meal! Great atmosphere and friendly staff. Will definitely return.",
                "stars": 4,
                "explanation": "Positive sentiment with specific praises for food, atmosphere, and service"
            },
            {
                "review": "Outstanding in every way! Best restaurant I've been to. Exceptional food, service, and ambiance. Highly recommend!",
                "stars": 5,
                "explanation": "Extremely positive with superlatives and strong recommendation"
            }
        ]

    def generate_prompt(self, review_text: str) -> str:
        examples_text = "\n\n".join([
            f"Example {i+1}:\nReview: \"{ex['review']}\"\nRating: {ex['stars']} stars\nExplanation: {ex['explanation']}"
            for i, ex in enumerate(self.examples)
        ])

        return f"""You are a rating prediction system. Learn from these examples and predict the star rating (1-5) for the new review.

{examples_text}

Now predict the rating for this review:
Review: "{review_text}"

Provide your prediction in JSON format:
{{
    "predicted_stars": <1-5>,
    "explanation": "<brief reasoning>"
}}"""


class ChainOfThoughtStrategy(PromptStrategy):
    """Chain-of-Thought Prompting - Step-by-step reasoning"""

    def __init__(self):
        super().__init__(
            name="Chain-of-Thought",
            description="Multi-step reasoning process for rating prediction"
        )

    def generate_prompt(self, review_text: str) -> str:
        return f"""You are a rating prediction system. Analyze the review step-by-step:

Review: "{review_text}"

Follow this reasoning process:
1. Identify the overall sentiment (positive, negative, neutral)
2. List specific positive aspects mentioned
3. List specific negative aspects mentioned
4. Assess the intensity of language (mild, moderate, strong)
5. Consider if there's a recommendation or anti-recommendation
6. Synthesize all factors to determine the star rating

Based on your step-by-step analysis, provide your prediction in JSON format:
{{
    "predicted_stars": <1-5>,
    "explanation": "<reasoning based on the steps above>"
}}"""


class RoleBasedStrategy(PromptStrategy):
    """Role-Based Prompting - Act as Fynd Feedback Manager"""

    def __init__(self):
        super().__init__(
            name="Role-Based (Fynd Manager)",
            description="Persona as Fynd's experienced feedback manager"
        )

    def generate_prompt(self, review_text: str) -> str:
        return f"""You are the Senior Feedback Manager at Fynd, India's leading omnichannel platform powering 1000+ brands.

Your expertise:
- 10+ years analyzing customer feedback across retail, fashion, and e-commerce
- Deep understanding of customer sentiment in the Indian market
- Expert at identifying quality issues, service gaps, and customer satisfaction drivers
- Trained to assess feedback for Fynd's partners including Reliance, Tata, and major retail brands

Context: Fynd Platform provides:
- Fynd Store (POS system for retail)
- Fynd Platform (omnichannel commerce)
- Fashion discovery and personalized shopping experiences

Task: Analyze this customer review as you would for a Fynd partner merchant. Consider factors critical to retail success: product quality, service excellence, delivery experience, and value perception.

Review: "{review_text}"

Apply your professional judgment and provide a rating prediction in JSON format:
{{
    "predicted_stars": <1-5>,
    "explanation": "<professional assessment from Fynd's perspective>"
}}"""


class PromptChainingStrategy(PromptStrategy):
    """Prompt Chaining - Multi-stage analysis"""

    def __init__(self, llm_client: LLMClient = None):
        super().__init__(
            name="Prompt Chaining",
            description="Two-stage analysis: sentiment extraction then rating prediction"
        )
        self.llm_client = llm_client

    def generate_prompt(self, review_text: str) -> str:
        return f"""You are a rating prediction system using a two-stage analysis approach.

Stage 1: First, analyze the review and extract sentiment features:
Review: "{review_text}"

Identify:
- Overall sentiment (positive/negative/neutral)
- Positive aspects mentioned
- Negative aspects mentioned
- Intensity of emotion (low/medium/high)
- Presence of recommendation

Stage 2: Based on your Stage 1 analysis, predict the star rating using these guidelines:
- 5 stars: Highly positive, strong recommendation, high intensity positive emotion
- 4 stars: Positive overall, may have minor issues, clear satisfaction
- 3 stars: Mixed or neutral, balanced positives and negatives
- 2 stars: Negative overall, significant issues, disappointment
- 1 star: Highly negative, strong complaints, anti-recommendation

Provide your final prediction in JSON format:
{{
    "predicted_stars": <1-5>,
    "explanation": "<reasoning based on both stages>"
}}"""


# ================================================================
# CELL 12: EvaluationFramework Class (FIXED VERSION)
# ================================================================
class EvaluationFramework:
    """Production-grade evaluation with comprehensive metrics"""

    def __init__(self, config: Config):
        self.config = config
        self.results = {}

    def evaluate_strategy(
        self,
        strategy: PromptStrategy,
        df: pd.DataFrame,
        llm_client: LLMClient
    ) -> Dict[str, Any]:
        """Evaluate a single prompting strategy"""

        print(f"\nüîÑ Evaluating: {strategy.name}")
        print(f"   {strategy.description}")

        predictions = []
        explanations = []
        json_valid_count = 0
        latencies = []

        for idx, row in df.iterrows():
            start_time = time.time()

            # Generate prompt and get prediction
            prompt = strategy.generate_prompt(row['text'])
            result = llm_client.call_llm(prompt)

            # Extract prediction with proper validation
            pred_stars = result.get('predicted_stars', 3)
            
            # Ensure it's an integer in valid range
            if pred_stars is None:
                pred_stars = 3
            try:
                pred_stars = int(pred_stars)
                pred_stars = max(1, min(5, pred_stars))
            except (ValueError, TypeError):
                pred_stars = 3
            
            predictions.append(pred_stars)
            explanations.append(result.get('explanation', 'N/A'))

            # Validate JSON structure
            if 'predicted_stars' in result and 'explanation' in result:
                json_valid_count += 1

            latencies.append(time.time() - start_time)

            # Progress indicator
            if (idx + 1) % 50 == 0:
                print(f"   Progress: {idx + 1}/{len(df)} reviews processed")

        # Calculate metrics with proper numpy arrays
        actual = df['stars'].values
        predictions_array = np.array(predictions, dtype=int)
        
        metrics = self._calculate_metrics(
            actual, predictions_array, json_valid_count, len(df), latencies
        )

        # Store results
        self.results[strategy.name] = {
            'strategy': strategy,
            'predictions': predictions,
            'explanations': explanations,
            'metrics': metrics,
            'confusion_matrix': confusion_matrix(actual, predictions_array, labels=[1, 2, 3, 4, 5])
        }

        print(f"   ‚úÖ Accuracy: {metrics['accuracy']:.2%}")
        print(f"   ‚úÖ JSON Validity: {metrics['json_validity_rate']:.2%}")
        print(f"   ‚úÖ MAE: {metrics['mae']:.3f}")

        return metrics

    def _calculate_metrics(
        self,
        actual: np.ndarray,
        predicted: np.ndarray,
        json_valid_count: int,
        total_count: int,
        latencies: List[float]
    ) -> Dict[str, float]:
        """Calculate comprehensive evaluation metrics"""

        # Ensure both arrays are the same length and type
        actual = np.array(actual, dtype=int)
        predicted = np.array(predicted, dtype=int)
        
        return {
            'accuracy': float(accuracy_score(actual, predicted)),
            'mae': float(mean_absolute_error(actual, predicted)),
            'json_validity_rate': float(json_valid_count / total_count),
            'avg_latency': float(np.mean(latencies)),
            'std_latency': float(np.std(latencies)),
            'within_1_star': float(np.mean(np.abs(actual - predicted) <= 1)),
            'exact_match': float(np.mean(actual == predicted))
        }

    def generate_comparison_table(self) -> pd.DataFrame:
        """Generate comparison table for all strategies"""

        comparison_data = []
        for name, result in self.results.items():
            metrics = result['metrics']
            comparison_data.append({
                'Strategy': name,
                'Accuracy': f"{metrics['accuracy']:.2%}",
                'MAE': f"{metrics['mae']:.3f}",
                'Within 1 Star': f"{metrics['within_1_star']:.2%}",
                'JSON Validity': f"{metrics['json_validity_rate']:.2%}",
                'Avg Latency (s)': f"{metrics['avg_latency']:.2f}"
            })

        return pd.DataFrame(comparison_data)

    def visualize_results(self):
        """Create comprehensive visualizations"""

        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('Prompting Strategies Evaluation - Production Analysis',
                    fontsize=16, fontweight='bold')

        strategies = list(self.results.keys())
        
        # 1. Accuracy Comparison
        accuracies = [self.results[s]['metrics']['accuracy'] for s in strategies]
        axes[0, 0].bar(range(len(strategies)), accuracies, color='steelblue')
        axes[0, 0].set_xticks(range(len(strategies)))
        axes[0, 0].set_xticklabels(strategies, rotation=45, ha='right')
        axes[0, 0].set_ylabel('Accuracy')
        axes[0, 0].set_title('Accuracy by Strategy')
        axes[0, 0].set_ylim([0, 1])
        for i, v in enumerate(accuracies):
            axes[0, 0].text(i, v + 0.02, f'{v:.2%}', ha='center')

        # 2. MAE Comparison
        maes = [self.results[s]['metrics']['mae'] for s in strategies]
        axes[0, 1].bar(range(len(strategies)), maes, color='coral')
        axes[0, 1].set_xticks(range(len(strategies)))
        axes[0, 1].set_xticklabels(strategies, rotation=45, ha='right')
        axes[0, 1].set_ylabel('Mean Absolute Error')
        axes[0, 1].set_title('MAE by Strategy (Lower is Better)')
        for i, v in enumerate(maes):
            axes[0, 1].text(i, v + 0.02, f'{v:.3f}', ha='center')

        # 3. JSON Validity Rate
        json_rates = [self.results[s]['metrics']['json_validity_rate'] for s in strategies]
        axes[0, 2].bar(range(len(strategies)), json_rates, color='mediumseagreen')
        axes[0, 2].set_xticks(range(len(strategies)))
        axes[0, 2].set_xticklabels(strategies, rotation=45, ha='right')
        axes[0, 2].set_ylabel('JSON Validity Rate')
        axes[0, 2].set_title('JSON Validity by Strategy')
        axes[0, 2].set_ylim([0, 1])
        for i, v in enumerate(json_rates):
            axes[0, 2].text(i, v + 0.02, f'{v:.2%}', ha='center')

        # 4. Confusion Matrix for Best Strategy
        best_strategy = max(strategies, key=lambda s: self.results[s]['metrics']['accuracy'])
        cm = self.results[best_strategy]['confusion_matrix']
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0],
                   xticklabels=[1,2,3,4,5], yticklabels=[1,2,3,4,5])
        axes[1, 0].set_xlabel('Predicted Stars')
        axes[1, 0].set_ylabel('Actual Stars')
        axes[1, 0].set_title(f'Confusion Matrix - {best_strategy}')

        # 5. Within 1 Star Accuracy
        within_1 = [self.results[s]['metrics']['within_1_star'] for s in strategies]
        axes[1, 1].bar(range(len(strategies)), within_1, color='mediumpurple')
        axes[1, 1].set_xticks(range(len(strategies)))
        axes[1, 1].set_xticklabels(strategies, rotation=45, ha='right')
        axes[1, 1].set_ylabel('Within 1 Star Rate')
        axes[1, 1].set_title('Predictions Within 1 Star of Actual')
        axes[1, 1].set_ylim([0, 1])
        for i, v in enumerate(within_1):
            axes[1, 1].text(i, v + 0.02, f'{v:.2%}', ha='center')

        # 6. Latency Comparison
        latencies = [self.results[s]['metrics']['avg_latency'] for s in strategies]
        axes[1, 2].bar(range(len(strategies)), latencies, color='gold')
        axes[1, 2].set_xticks(range(len(strategies)))
        axes[1, 2].set_xticklabels(strategies, rotation=45, ha='right')
        axes[1, 2].set_ylabel('Avg Latency (seconds)')
        axes[1, 2].set_title('Average Response Time')
        for i, v in enumerate(latencies):
            axes[1, 2].text(i, v + 0.05, f'{v:.2f}s', ha='center')

        plt.tight_layout()
        plt.savefig('prompting_evaluation.png', dpi=300, bbox_inches='tight')
        plt.show()


# ================================================================
# CELL 13: Main Evaluation Pipeline
# ================================================================
def run_evaluation_pipeline(df: pd.DataFrame) -> Dict[str, Any]:
    """Main execution pipeline for production evaluation"""

    print("="*80)
    print("üöÄ STARTING PRODUCTION-GRADE EVALUATION PIPELINE")
    print("="*80)

    # Initialize components
    llm_client = LLMClient(config)
    evaluator = EvaluationFramework(config)

    # Define strategies
    strategies = [
        ZeroShotStrategy(),
        FewShotStrategy(),
        ChainOfThoughtStrategy(),
        RoleBasedStrategy(),
        PromptChainingStrategy(llm_client)
    ]

    print(f"\nüìã Evaluating {len(strategies)} prompting strategies on {len(df)} reviews\n")

    # Evaluate each strategy
    for strategy in strategies:
        evaluator.evaluate_strategy(strategy, df, llm_client)

    # Generate comparison table
    print("\n" + "="*80)
    print("üìä COMPREHENSIVE COMPARISON TABLE")
    print("="*80 + "\n")
    comparison_df = evaluator.generate_comparison_table()
    print(comparison_df.to_string(index=False))

    # Visualize results
    print("\n" + "="*80)
    print("üìà GENERATING VISUALIZATIONS")
    print("="*80)
    evaluator.visualize_results()

    # LLM metrics
    print("\n" + "="*80)
    print("üîß LLM CLIENT METRICS")
    print("="*80)
    llm_metrics = llm_client.get_metrics_summary()
    for key, value in llm_metrics.items():
        print(f"   {key}: {value}")

    # Best strategy analysis
    best_strategy = max(
        evaluator.results.keys(),
        key=lambda s: evaluator.results[s]['metrics']['accuracy']
    )

    print("\n" + "="*80)
    print("üèÜ BEST PERFORMING STRATEGY")
    print("="*80)
    print(f"   Strategy: {best_strategy}")
    print(f"   Accuracy: {evaluator.results[best_strategy]['metrics']['accuracy']:.2%}")
    print(f"   MAE: {evaluator.results[best_strategy]['metrics']['mae']:.3f}")
    print(f"   JSON Validity: {evaluator.results[best_strategy]['metrics']['json_validity_rate']:.2%}")

    print("\n" + "="*80)
    print("‚úÖ EVALUATION PIPELINE COMPLETED SUCCESSFULLY")
    print("="*80 + "\n")

    return {
        'comparison_table': comparison_df,
        'best_strategy': best_strategy,
        'all_results': evaluator.results,
        'llm_metrics': llm_metrics
    }


# ================================================================
# CELL 14: Sample Prediction Analysis
# ================================================================
def analyze_sample_predictions(results: Dict, df: pd.DataFrame, n_samples: int = 5):
    """Analyze sample predictions from each strategy"""

    print("\n" + "="*80)
    print("üîç SAMPLE PREDICTION ANALYSIS")
    print("="*80 + "\n")

    sample_indices = np.random.choice(len(df), n_samples, replace=False)

    for idx in sample_indices:
        print(f"\n{'='*80}")
        print(f"Review #{idx + 1}")
        print(f"{'='*80}")
        print(f"\nText: {df.iloc[idx]['text'][:200]}...")
        print(f"\nActual Rating: {df.iloc[idx]['stars']} stars")
        print(f"\nPredictions:")

        for strategy_name, result in results['all_results'].items():
            pred = result['predictions'][idx]
            expl = result['explanations'][idx]
            print(f"\n  {strategy_name}:")
            print(f"    Predicted: {pred} stars")
            print(f"    Explanation: {expl}")


# ================================================================
# CELL 15: Export Production Artifacts
# ================================================================
def export_production_artifacts(results: Dict):
    """Export artifacts for production deployment"""

    # 1. Save best strategy configuration
    best_strategy_name = results['best_strategy']
    config_export = {
        'best_strategy': best_strategy_name,
        'model': config.MODEL_NAME,
        'temperature': config.TEMPERATURE,
        'max_tokens': config.MAX_TOKENS,
        'performance_metrics': results['all_results'][best_strategy_name]['metrics']
    }

    with open('production_config.json', 'w') as f:
        json.dump(config_export, f, indent=2, default=str)

    print("‚úÖ Production configuration exported to 'production_config.json'")

    # 2. Save prompt templates
    prompt_templates = {}
    for strategy_name, result in results['all_results'].items():
        strategy = result['strategy']
        prompt_templates[strategy_name] = {
            'description': strategy.description,
            'sample_prompt': strategy.generate_prompt("Sample review text")
        }

    with open('prompt_templates.json', 'w') as f:
        json.dump(prompt_templates, f, indent=2)

    print("‚úÖ Prompt templates exported to 'prompt_templates.json'")

    # 3. Save evaluation report
    report = {
        'evaluation_date': datetime.now().isoformat(),
        'dataset_size': config.SAMPLE_SIZE,
        'strategies_
